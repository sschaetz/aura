Backend CUDA
============
1) device is created from id (or whatever)
2) context is created from device (context is associated with calling thread 
right away)
3) after that, cuda commands can be launched 
4) streams seem to specific to contexts (check this), so a stream should be 
created from a device/context

streams and events are per device

Devices, Streams, and Events
* CUDA streams and events are per device 
    Determined by the GPU thatâ€™s current at the time of their creation
    Each device has its own default stream (aka 0- or NULL-stream)
* Using streams and events 
    Calls to a stream can be issued only when its device is current
    Event can be recorded only to a stream of the same device

For peer 2 peer communication, which context needs to be specified (source or
destination)?

Popping the context every time ca be a bad idea when doing low level
programming. A tool should be provided to make a feed permanent.

For CUDA to release memory, the correct context must be set first.


Backend OpenCL
==============

0) opencl has platforms (since there can exist Intel, AMD and Nvidia cards in 
the same system
1) device is created from id (or whatever) and platform
2) a context is created from a device
3) a queue is created from context and device
4) after that, opencl commands can be launched in the queue

devices can share a context
devices can not share a command queue
this slidedeck [0] shows some advantages of 1 context per device over multiple 
devices in context so maybe we should do that 
[0] https://www.cvg.ethz.ch/teaching/2011spring/gpgpu/GPU-multi-device.pdf

let's ignore platforms for now, let's offer all available devices
devices are created and contain a context each and a default command queue (a stream)
streams are additional command queues
devices and command queues can be set as defaults per thread

command queues are per device

